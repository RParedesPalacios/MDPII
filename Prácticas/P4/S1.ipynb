{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "492bc8de",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <h3>Modelos Descriptivos y Predictivos II - Grado en Ciencia de Datos</h3>\n",
    "    <h3>Universitat Politècnica de València</h3>\n",
    "    <h1>Práctica 4: Máquinas de Soporte Vectorial (SVM)</h1>\n",
    "    <h2>Sesión 1</h2>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17baa2da",
   "metadata": {},
   "source": [
    "### Objetivos formativos:\n",
    "\n",
    "- Entender los conceptos fundamentales del clasificador SVM\n",
    "- Analizar distintas tareas creadas artificialmente mediante datasets sintéticos.\n",
    "- Evaluar el efecto de distintos kernels en distintas tareas\n",
    "- Evaluar el efecto de los parámetros C y gamma\n",
    "- Encontrar los hiperpará,metros óptimos para una tarea determinada"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17340e96",
   "metadata": {},
   "source": [
    "### Importación de módulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e387628",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import make_moons, make_blobs, make_circles, load_wine, load_breast_cancer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score, roc_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d1f26b",
   "metadata": {},
   "source": [
    "### Creación de datasets artificiales\n",
    "\n",
    "En esta práctica usaremos algunos conjuntos de datos creados de manera artificial. A continuación se da el código de distintas funciones para crear estos datasets:\n",
    "- `create_dataset_linear`: genera puntos n-dimensionales en dos grupos (blobs) con centroides en (-1,-1,...,-1) y (1,1,...,1).<br>**Parámetros:**\n",
    "    - `samples`: número de puntos a generar.\n",
    "    - `std`: dispersión (desviación estándar) de los puntos en cada grupo. Puede ser un `float` (todos los grupos tienen la misma dispersión) o un array (permite especificar una dispersión distinta para cada grupo).\n",
    "    - `n_features`: dimensión de los puntos generados.\n",
    "- `create_dataset_moons`: genera puntos 2D en dos grupos con formas de media luna.<br>**Parámetros:**\n",
    "    - `samples`: número de puntos a generar.\n",
    "    - `noise`: ruido de las muestras. A mayor ruido, mayor solapamiento entre las muestras de ambos grupos.\n",
    "\n",
    "- `create_dataset_poly`: genera puntos 2D en dos grupos separados por una frontera poligonal<br>**Parámetros:**\n",
    "    - `coefs`: coeficientes del polígono que define la frontera.\n",
    "    - `samples`: número de puntos a generar.\n",
    "    - `noise`: ruido de las muestras. A mayor ruido, mayor solapamiento entre las muestras de ambos grupos.\n",
    "    - `box`: tamaño del cuadrado en el que se enmarcan los puntos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10aee56",
   "metadata": {},
   "source": [
    "**Funciones generadoras de datos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8bab3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_linear(samples=300, std=0.5, n_features=2): \n",
    "    # Generar un conjunto de datos linealmente separable\n",
    "    X, y = make_blobs(n_samples=samples, centers=[[-1]*n_features, [1]*n_features] , n_features=n_features, cluster_std=std, random_state=17)\n",
    "    return X, y\n",
    "\n",
    "def create_dataset_moons(samples=300, noise=0.1):\n",
    "    # Generar un conjunto de datos con clases en forma de dos lunas\n",
    "    X, y = make_moons(n_samples=samples, noise=noise, random_state=32)\n",
    "    return X, y\n",
    "\n",
    "def create_dataset_poly(coefs, samples=300, noise=0.1, box=20):\n",
    "    np.random.seed(32)\n",
    "    X = np.random.uniform(-box/2, box/2, (samples, 2))\n",
    "    y = np.zeros(samples)\n",
    "    grade = len(coefs)-1\n",
    "    for i, p in enumerate(X):\n",
    "        px,py = p[0],p[1]\n",
    "        y_poly = sum(coef * (px ** (grade-j)) for j, coef in enumerate(coefs))\n",
    "        y_noisy = y_poly + np.random.uniform(-noise, noise)*box\n",
    "        y[i] = 0 if y_noisy < py else 1\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c20dc7b",
   "metadata": {},
   "source": [
    "**Crear datasets y plotear**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4a874d",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(nrows=1, ncols=3, figsize=(15, 5))\n",
    "\n",
    "X, y = create_dataset_linear(std=[0.75,0.75])\n",
    "ax[0].scatter(X[:, 0], X[:, 1], c=y)\n",
    "ax[0].set_title('Linear')\n",
    "\n",
    "X, y = create_dataset_moons(noise=0.2)\n",
    "ax[1].scatter(X[:, 0], X[:, 1], c=y)\n",
    "ax[1].set_title('Moons')\n",
    "\n",
    "X, y = create_dataset_poly([0.1,0,-7], noise=0.1)\n",
    "ax[2].scatter(X[:, 0], X[:, 1], c=y)\n",
    "ax[2].set_title('Polygonal')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafe25c6",
   "metadata": {},
   "source": [
    "### Ejercicio 1. Separación de clases: fronteras de decisión, margen, vectores soporte y kernel \n",
    "\n",
    "El **margen** es la distancia entre el hiperplano de separación y los puntos de datos más cercanos de cada clase. El objetivo de SVM es maximizar este margen. En este ejercicio vamos a plotear:\n",
    "- Los márgenes (línia discontinua)\n",
    "- El hiperplano de separación (línea continua)\n",
    "- Los vectores soporte (circunferencias rojas)\n",
    "- La predicción del modelo (fondo de color)\n",
    "\n",
    "El código que se muestra a continuación crea un dataset artificial con alguna de las funciones dadas, lo clasifica con un clasificador SVM y muestra los resultados.\n",
    "\n",
    "**Se pide** ejecutar el código con cada uno de los datasets propuestos (descomentanto el que se desee evaluar) y observar los resultados obtenidos. Se deberá probar, en cada caso, el kernel ('linear', 'poly', 'rbf') que mejor se adapta al problema. \n",
    "\n",
    "A continuación, anota en la tabla de más abajo, para cada dataset, el tipo de kernel y el parámetro C que mejores resultados ofrece, junto con el *accuracy* obtenido.\n",
    "\n",
    "**Resultados:**\n",
    "\n",
    "|Dataset| kernel |  C  | accuracy |\n",
    "|-------|--------|-----|----------|\n",
    "|linear |        |     |          |\n",
    "|moons  |        |     |          |\n",
    "|poly   |        |     |          |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0f2b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Descomenta el dataset que quieras evaluar\n",
    "X, y = create_dataset_linear(samples=20, std=[0.65,0.65])  # Dataset lineal\n",
    "# X, y = create_dataset_moons(samples=150, noise=0.3)         # Dataset moons \n",
    "# X, y = create_dataset_poly([0.1,0,-7], samples=200, noise=0.4)  # Dataset polygon \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=23)\n",
    "\n",
    "C = [0.1,1,10]\n",
    "_, axes = plt.subplots(nrows=1, ncols=len(C), figsize=(15, 5))\n",
    "\n",
    "for c,ax in zip(C,axes):\n",
    "    # Crear y entrenar el modelo SVM\n",
    "    model = SVC(kernel='linear', C=c, gamma='auto')\n",
    "    scaler = StandardScaler()\n",
    "    pipe = Pipeline([(\"scaler\", scaler), (\"svm\", model)])\n",
    "    acc = pipe.fit(X_train, y_train).score(X_test, y_test)\n",
    "\n",
    "    # Mostrar puntos de datos\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y)\n",
    "\n",
    "    # Mostrar regiones de decisión\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))\n",
    "    Z = pipe.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    ax.contourf(xx, yy, Z, alpha=0.2)\n",
    "\n",
    "    # Mostrar márgenes y frontera de decisión\n",
    "    ax.contour(xx, yy, Z, levels=[-1,0,1], linestyles=['--', '-', '--'], linewidths=[1,2,1])  # Línea de frontera y márgenes\n",
    "\n",
    "    # Mostrar vectores soporte\n",
    "    support_vectors = scaler.inverse_transform(model.support_vectors_)\n",
    "    ax.scatter(support_vectors[:, 0], support_vectors[:, 1], s=100, facecolors='none', edgecolors='red', linewidths=1.5, label=\"Vectores de soporte\")\n",
    "\n",
    "    # Título\n",
    "    ax.set_title(f'C={c}   acc.={acc:.2f}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df60ab5c",
   "metadata": {},
   "source": [
    "### Ejercicio 2. Parámetro de regularización C\n",
    "\n",
    "El parámetro **C** representa cómo el modelo penaliza los errores en la clasificación de los datos de entrenamiento. Esto permite controlar el compromiso entre el ajuste del modelo y la generalización del mismo.\n",
    "\n",
    "- Valores de C altos (C > 1) penaliza los errores de clasificación, con lo que el modelo intenta clasificar un mayor número de datos de entrenamiento correctamente. Tiene la ventaja de **minimizar los errores** en el conjunto de entrenamiento y la desventaja de tener un **mayor riesgo de sobreajuste**.\n",
    "\n",
    "- Valores de C bajos (0< C < 1) da mayor tolerancia a errores de clasificación en los datos de entrenamiento, produciendo fronteras de decisión más sencillas. Esto **favorece la generalización**, pero conlleva un **mayor riesgo de subajuste**.\n",
    "\n",
    "En este ejercicio vamos a crear un dataset de tipo `linear` con 150 observaciones, 10 características y `std=1` para la primera clase y `std=3` para la segunda. Esto creará una dataset con una frontera lineal, pero con bastante solapamiento entre los puntos de ambas clases (devido a la desviación estándar alta). Además tiene pocas observaciones, con lo que hay un mayor riesgo de sobreajuste.\n",
    "\n",
    "El objetivo es encontrar el parámetro de regularización C óptimo para este dataset, utilizando para ello la técnica de **Grid Search**. Dado que vamos a usar con frecuencia la clase `GridSearchCV`, a continuación se da una función que muestra los resultados obtenidos con un objeto de este tipo. Puedes utilizar esta función en tus ejercicios para mostrar los resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9c4a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_results_gs(gs, sort=False):\n",
    "    # Extraer los resultados del GridSearchCV\n",
    "    results = gs.cv_results_\n",
    "\n",
    "    # Crear un DataFrame con los parámetros y los resultados medios\n",
    "    df_results = pd.DataFrame({\n",
    "        'mean_test_score': results['mean_test_score'],\n",
    "        'params': results['params']\n",
    "    })\n",
    "\n",
    "    # Ordenar los resultados por el resultado promedio (opcional)\n",
    "    if sort:\n",
    "        df_results = df_results.sort_values(by='mean_test_score', ascending=False)\n",
    "\n",
    "    # Mostrar los resultados\n",
    "    print(df_results)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7dac5d",
   "metadata": {},
   "source": [
    "Escribe a continuación tu código:\n",
    "- Encuentra el parámetro de regularización C óptimo. \n",
    "- Muestra los resultados obtenidos con el conjunto de entrenamiento (usando la función `show_results_gs`\n",
    "- Muestra el valor de C óptimo\n",
    "- Muestra el resultado obtenido en el conjunto de test con el mejor clasificador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a21dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ESCRIBE AQUÍ TU SOLUCIÓN \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f10e1a",
   "metadata": {},
   "source": [
    "### Ejercicio 3. Parámetro gamma (kernel RBF)\n",
    "\n",
    "Basándote en el código que se da en el ejercicio 1, realiza el siguiente experimento:\n",
    "\n",
    "- Crea un dataset de tipo *moons* con 100 muestras y ruido=0.2\n",
    "- Ajusta un clasificador SVC con kernel RBF, C=1 y valores de gamma = [0.1, 0.5, 1, 5]. Muestra gráficas similares a las del ejercicio 1\n",
    "- Observa el efecto que tiene el parámetro gamma en la definición de la frontera de decisión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184caff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ESCRIBE AQUÍ TU SOLUCIÓN \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MDPII",
   "language": "python",
   "name": "mdp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
