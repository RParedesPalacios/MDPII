{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objetivos formativos\n",
    "- Diseñar un experimento de clasificación con un dataset de imágenes\n",
    "- Implementar clasificadores basados redes neuronales multicapa con Keras\n",
    "- Variar algunos de los parámetros principales de la red neuronal\n",
    "- Guardar la red neuronal para poder cargar el modelo y realizar posterior inferencia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MultiLayer Perceptron con el conjunto de datos MNIST\n",
    "\n",
    "MNIST es un conjunto de datos que representa una tarea de clasificación de pequeñas imágenes (28x28 píxeles). Las imágenes se agrupan en 10 clases, los dígitos del 0 al 9.\n",
    "\n",
    "Aunque es una tarea de clasificación de imágenes vamos a emplear topologías de redes neuronales MLP en lugar de topologías más avanzadas basadas en redes convolucionales. \n",
    "\n",
    "El motivo de usar MNIST es que se trata de un conunto de datos ampliamente empleado por lo que el alumno podrá encontrar muchas referencias e incluso compararse con el estado del arte en esta tarea.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imágenes de MNIST\n",
    "\n",
    "Un ejemplo de una de las imágenes del conjunto de datos disponible:\n",
    "\n",
    "![ejemplo mnist](assets/mnist.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representación lineal de las imágenes\n",
    "\n",
    "Se requiere una representación lineal (1D) de las imágenes para poder emplearlas como datos de entrada en un MLP. Para ellos lo usual es concatenar cada una de las filas de la imagen convirtiendo el tensor bidimensional (28x28) en un tensor unidimensional (784).\n",
    "\n",
    "![MNIST](assets/mnist.gif \"MNIST\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lectura de los datos y conversión con **Keras**\n",
    "\n",
    "Vamos a implemetar la importación de este conjunto de datos y la conversión en tensores unidimensionales. Para ello Keras y numpy nos proveen de una serie de utilidades. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ASUMIENDO KERAS V2\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.datasets import mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "print('training set', x_train.shape)\n",
    "print('test set', x_test.shape)\n",
    "\n",
    "import numpy as np\n",
    "print(np.max(x_train)) # <-- comprobar que las imágenes vienen normalizadas entre 0 y 255, niveles de gris de 8 bits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importante resaltar que al importar MNIST tenemos en realidad dos conjuntos de datos: *training* y *test*. Además, por cada conjunto de datos tenemos dos arrays, uno con los datos (imágenes) y otro con las etiquetas (clases) asociadas a dichas imágenes.\n",
    "\n",
    "A continuación realizamos algunas modificaciones necesarias: \n",
    "\n",
    "1. convertimos los arrays a 1D\n",
    "2. normalizamos los valores para que estén en el intervalo [0..1]\n",
    "3. convertimos las etiquetas de clase en un one-hot vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(60000, 784)\n",
    "x_test = x_test.reshape(10000, 784)\n",
    "x_train = x_train.astype('float32') # <-- convertir a float32 para que la normalización sea en coma flotante\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "# Normalize [0..255]-->[0..1]\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "num_classes=10\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con este código estamos listos para crear las primeras redes neuronales MLP y entrenar los modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primeros modelos KERAS sobre MNIST\n",
    "\n",
    "Vamos a implementar las primeras redes neuronales para el conjunto de datos MNIST\n",
    "\n",
    "IMPORTAMOS el Dataset y lo normalizamos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras.datasets import mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "print('training set', x_train.shape)\n",
    "print('test set', x_test.shape)\n",
    "\n",
    "x_train = x_train.reshape(60000, 784)\n",
    "x_test = x_test.reshape(10000, 784)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "# Normalize [0..255]-->[0..1]\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "num_classes=10\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos secuenciales en KERAS\n",
    "\n",
    "Los modelos secuenciales son modelos donde las capas se van añadiendo una tras otra. Te permite crear topologías básicas sin conexiones que no sean estrictamente lineales.\n",
    "\n",
    "La red neuronal a crear es la siguiente:\n",
    "\n",
    "- Capa de entrada de 784 neuronas, acorde con la dimensionalidad de los datos\n",
    "- Capa oculta de 512 neuronas, con función de activación ReLU\n",
    "- Capa de salida de 10 neuronas, acorde con el número de clases. Al ser un problema de clasificación emplearemos la función de activación Softmax\n",
    "\n",
    "Este sería la definición del modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import Dense, Input\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Input((784,)))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![model summary](assets/summary.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El summary del modelo nos lista las diferentes capas del mismo así como su número de parámetros. Por ejemplo la capa densa oculta tiene 784 entradas x 512 salidas lo que implica una matriz de 784x512 = 401408 componentes. Además esta capa Dense tiene un vector de bias de 512 componentes, por lo tanto en total son 401408+512 = **401920**, que coincide con el número de parámetros de la tabla para dicha capa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos generar una imagen con la topología del modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.utils.plot_model(model, to_file=\"model.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compilar el modelo \n",
    "\n",
    "Para terminar la definición de nuestro modelo debemos definir dos componentes muy importantes:\n",
    "\n",
    "1. Función de pérdida. En nuestro caso al ser un problema de clasificación emplearemos la **categorical_crossentropy**\n",
    "2. Optimizador. En nuestro caso y para empezar emplearemos un sencillo descenso por gradiente estocástico **SGD**\n",
    "\n",
    "opcionalmente:\n",
    "\n",
    "3. Definir una métrica asociada a la calidad del modelo. En nuestro caso sería la tasa de acierto **accuracy**\n",
    "\n",
    "\n",
    "Una vez definidos pasamos a compilar el modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD\n",
    "\n",
    "sgd=SGD(learning_rate=0.01, momentum=0.9)\n",
    "\n",
    "# Compile Model\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenar el modelo\n",
    "\n",
    "Una vez tenemos los datos cargados y normalizados, así como el modelo ya compilado, podemos realizar el entrenamiento mediante el método **fit**. Para ello previamente necesitamos definir el tamaño del batch así como el número de epochs.\n",
    "\n",
    "Al mismo tiempo que entrenamos el modelo con los datos de entrenamiento vamos a ir evaluando dicho modelo sobre los datos de test. Además, vamos generando un **history** con la evolución del modelo para luego poder crear gráficas del mismo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=32\n",
    "epochs=25\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explorar el History\n",
    "\n",
    "Realizar gráfica con resultados de accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history.history.keys())\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizar gráfica con resultados de loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Ejercicio** \n",
    "\n",
    "probar diferentes entrenamientos dependiendo de los siguientes valores y anotar el accuracy en test alcanzado\n",
    "\n",
    "| Dense/Batch  | 16  | 32  |  64 | 128  |\n",
    "|---|---|---|---|---|\n",
    "|  256  |   |   |   |   |\n",
    "|  512  |   |   |   |   |\n",
    "|  1024 |   |   |   |   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Salvar y cargar el modelo\n",
    "\n",
    "Normalmente necesitaremos salvar el modelo entrenado para emplearlo más tarde en producción (inferencia)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar datos y normalizar\n",
    "from tensorflow import keras\n",
    "from keras.datasets import mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "print('training set', x_train.shape)\n",
    "print('test set', x_test.shape)\n",
    "\n",
    "x_train = x_train.reshape(60000, 784)\n",
    "x_test = x_test.reshape(10000, 784)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "num_classes=10\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "# crear modelo y entrenar\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, Input\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Input((784,)))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "sgd=SGD(learning_rate=0.01, momentum=0.9)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "batch_size=32\n",
    "epochs=25\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test))\n",
    "\n",
    "\n",
    "# guardar modelo\n",
    "model.save(\"model.keras\")\n",
    "\n",
    "# cargar modelo\n",
    "from keras.models import load_model\n",
    "model = load_model(\"model.keras\")\n",
    "\n",
    "# evaluar modelo\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "397704579725e15f5c7cb49fe5f0341eb7531c82d19f2c29d197e8b64ab5776b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
