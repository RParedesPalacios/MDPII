{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objetivos formativos\n",
    "- Diseñar un experimento de clasificación mediante el vecino más cercano\n",
    "- Analizar los resultados obtenidos en un problema de clasificación\n",
    "- Realizar una búsqueda de los mejores parámetros \n",
    "- Introducir nuevas funcionalidades de sklearn como la validación cruzada y la partición de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iris dataset\n",
    "\n",
    "Descargamos el dataset y dibujamos empleando dos características:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:,[2,3]]\n",
    "Y = iris.target\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "ax.axis([0.5, 7.5, -0.25, 3])\n",
    "ax.scatter(X[0:49,0], X[0:49,1], color=\"magenta\", marker=\"d\") \n",
    "ax.scatter(X[50:99,0], X[50:99,1], color=\"r\", marker=\"v\") \n",
    "ax.scatter(X[100:149,0], X[100:149,1], color=\"b\", marker=\"*\") \n",
    "ax.set_title(\"Iris\")\n",
    "\n",
    "ax.set_xlabel(\"Longitud de pétalo\")\n",
    "ax.set_ylabel(\"Anchura de pétalo\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Empleamos el vecino más próximo\n",
    "\n",
    "Vemos las fronteras de decisión del vecino más próximo con estos datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "KNNexample = KNeighborsClassifier(n_neighbors=1,weights=\"distance\")\n",
    "KNNexample.fit(X, Y)\n",
    "\n",
    "y1_min, y1_max = 0.5, 7.5\n",
    "y2_min, y2_max = -0.25, 3\n",
    "\n",
    "xx, yy = np.meshgrid(np.arange(y1_min, y1_max, 0.1),np.arange(y2_min, y2_max, 0.1))\n",
    "\n",
    "f, axarr = plt.subplots()\n",
    "Z = KNNexample.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "axarr.contourf(xx, yy, Z, alpha=0.4)\n",
    "axarr.scatter(X[0:49,0], X[0:49,1], color=\"magenta\",marker=\"d\")\n",
    "axarr.scatter(X[50:99,0], X[50:99,1], color=\"r\",marker=\"v\")\n",
    "axarr.scatter(X[100:149,0], X[100:149,1], color=\"b\",marker=\"*\")\n",
    "axarr.set_title(\"KNN (K=1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dividir el conjunto de datos entre training y test y clasificar\n",
    "\n",
    "Vamos a dividir el conjunto de datos en entrenamiento y test. A continuación emplearemos el entrenamiento para definir un clasificador y el de test para evaluarlo sobre dicho clasificador.\n",
    "\n",
    "Además vamos a emplear ya las 4 variables que hay en Iris.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "Y = iris.target\n",
    "\n",
    "## Partición train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, shuffle=True, random_state=12)\n",
    "\n",
    "\n",
    "## Entrenamiento\n",
    "kv = KNeighborsClassifier()\n",
    "knn=kv.fit(X_train,y_train)\n",
    "\n",
    "## Evaluación sobre el test\n",
    "acc=knn.score(X_test,y_test)\n",
    "\n",
    "\n",
    "print(f'Precisión: {acc:.1%}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Realizar validación cruzada para búsqueda de parámetros\n",
    "\n",
    "Cuando tenemos tan pocos datos como es el caso de Iris lo ideal es realizar una validación cruzada para obtener una mejor estimación del error de clasificación.\n",
    "\n",
    "Además vamos a variar el parámetro \"K\" de los K-vecinos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "Y = iris.target\n",
    "\n",
    "neighbors = list(range(1,10,2))\n",
    "cv_scores = [ ]\n",
    "for K in neighbors:\n",
    "    knn = KNeighborsClassifier(n_neighbors = K, weights=\"distance\")\n",
    "    scores = cross_val_score(knn,X,Y,cv = 5,scoring =\"accuracy\")\n",
    "    cv_scores.append(scores.mean())\n",
    "    \n",
    "\n",
    "optimal_k = neighbors[cv_scores.index(max(cv_scores))]\n",
    "print(\"The optimal no. of neighbors is {}\".format(optimal_k))\n",
    "\n",
    "def plot_accuracy(knn_list_scores):\n",
    "    pd.DataFrame({\"K\":[i for i in range(1,10,2)],\n",
    "      \"Accuracy\":knn_list_scores}).set_index(\"K\").plot.bar(figsize= (9,6), ylim=(0.8,1.0),rot=0)\n",
    "    plt.show()\n",
    "\n",
    "plot_accuracy(cv_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validación cruzada con GridSearch\n",
    "\n",
    "Una forma muy adecuada de implementar la validación cruzada, sobre todo cuando tenemos muchos parámetros que evaluar es emplear las utilidades que ya tiene sklearn. Como por ejemplo GridSearch. En este caso se nos devuelve siempre el mejor clasificador con el parámetro que mejor resultado ha dado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Creamos un diccionario con todos los parámetros a modificar, en este caso sólo uno\n",
    "G = {\"n_neighbors\":range(1,10,2)}  # n_neighbors es el nombre del parámetro\n",
    "\n",
    "GS = GridSearchCV(KNeighborsClassifier(), G, scoring='accuracy', refit=True, cv=5) # validación cruzada de 5 particiones\n",
    "\n",
    "knn = GS.fit(X_train, y_train)\n",
    "print(GS.cv_results_[\"params\"])\n",
    "print(GS.cv_results_[\"mean_test_score\"])\n",
    "\n",
    "acc= knn.score(X_test, y_test)\n",
    "print(f'Precisión: {acc:.1%} con {GS.best_params_}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación del dataset Digits\n",
    "\n",
    "Emplearemos ahora el dataset DIGITS, imágenes de 8x8 píxeles de los 10 dígitos $[0,9]$. En total hay 1797 imágenes de 8x8 píxeles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = datasets.load_digits()\n",
    "\n",
    "_, axes = plt.subplots(nrows=1, ncols=10, figsize=(10, 3))\n",
    "for ax, image, label in zip(axes, digits.images, digits.target):\n",
    "    ax.set_axis_off()\n",
    "    ax.imshow(image, cmap=plt.cm.gray_r, interpolation=\"nearest\")\n",
    "    ax.set_title(\"%i\" % label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio\n",
    "\n",
    "Realizar una validación cruzada para encontrar tanto el mejor valor del parámetro K como para el parámetro weights para el que hay que probar dos opciones: \"distance\" ó \"uniform\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solución"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(digits.images.shape)\n",
    "\n",
    "n_samples = len(digits.images)\n",
    "X=digits.images.reshape(n_samples,-1)\n",
    "Y=digits.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, shuffle=True, random_state=12)\n",
    "\n",
    "# Creamos un diccionario con todos los parámetros a modificar\n",
    "G = {\"n_neighbors\":range(1,10,2), \"weights\":[\"distance\",\"uniform\"]}  \n",
    "\n",
    "GS = GridSearchCV(KNeighborsClassifier(), G, scoring='accuracy', refit=True, cv=5) # validación cruzada de 5 particiones\n",
    "\n",
    "knn = GS.fit(X_train, y_train)\n",
    "print(GS.cv_results_[\"params\"])\n",
    "print(GS.cv_results_[\"mean_test_score\"])\n",
    "\n",
    "acc= knn.score(X_test, y_test)\n",
    "print(f'Precisión: {acc:.1%} con {GS.best_params_}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.10 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a3d66e091dbc1585363e043a3a2f918bc184b70b8d651cbd15f0740c4f1f7401"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
