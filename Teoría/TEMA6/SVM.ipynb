{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9608b463",
   "metadata": {},
   "source": [
    "\n",
    "# Enunciado del Problema\n",
    "\n",
    "Dado un conjunto de datos de entrenamiento linealmente separables en dos dimensiones, implementaremos un modelo SVM para encontrar el hiperplano óptimo que maximice el margen entre dos clases. \n",
    "Los pasos incluyen:\n",
    "\n",
    "1. Definir los puntos de entrenamiento y sus etiquetas.\n",
    "2. Resolver el problema dual de optimización cuadrática para obtener los multiplicadores de Lagrange (α).\n",
    "3. Identificar los vectores soporte (aquellos con α > 0).\n",
    "4. Calcular el vector de pesos (θ) y el término independiente (b) del modelo.\n",
    "5. Clasificar una nueva muestra [1, 1] usando el modelo entrenado.\n",
    "6. Visualizar la frontera de decisión y los vectores soporte.\n",
    "\n",
    "Los datos de entrenamiento son:\n",
    "\n",
    "- **Clase +1 (y = +1)**: [2, 3], [3, 3], [2, 2]\n",
    "- **Clase -1 (y = -1)**: [0, 1], [1, 0], [0, 0]\n",
    "\n",
    "Este ejercicio utiliza la librería `cvxopt` para resolver el problema de optimización cuadrática y obtener los multiplicadores de Lagrange de forma precisa, cumpliendo con las condiciones de Karush-Kuhn-Tucker (KKT) para los puntos que están en el margen. El objetivo es obtener una clasificación óptima para datos linealmente separables y observar el comportamiento de los vectores soporte.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044b59ae",
   "metadata": {},
   "source": [
    "# Ejercicio SVM con Frontera de Decisión y Vectores Soporte"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9afd4e3",
   "metadata": {},
   "source": [
    "\n",
    "Este notebook resuelve un ejercicio de clasificación utilizando un SVM en Python. Los datos de ejemplo tienen dos clases linealmente separables y se utiliza `cvxopt` para resolver el problema de optimización cuadrática, obteniendo los multiplicadores de Lagrange, el vector de pesos, y el término independiente para la frontera de decisión. Luego, se clasifica una nueva muestra y se visualizan los resultados.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedafdda",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Importaciones necesarias\n",
    "import numpy as np\n",
    "import cvxopt\n",
    "import cvxopt.solvers\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51836581",
   "metadata": {},
   "source": [
    "## Definición de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ea126c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Definimos las muestras y sus etiquetas\n",
    "X = np.array([[2, 3], [3, 3], [2, 2], [0, 1], [1, 0], [0, 0]])\n",
    "y = np.array([1, 1, 1, -1, -1, -1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ad3e7b",
   "metadata": {},
   "source": [
    "## Configuración y resolución del problema dual usando `cvxopt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424112ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convertimos las etiquetas en formato columna para la optimización\n",
    "Y = y[:, np.newaxis] * 1.0\n",
    "\n",
    "# Calculamos la matriz de Gram (producto punto entre muestras) escalada por etiquetas\n",
    "K = np.outer(y, y) * np.dot(X, X.T)\n",
    "\n",
    "# Configuramos los parámetros del problema de optimización cuadrática\n",
    "m, n = X.shape\n",
    "P = cvxopt.matrix(K, tc='d')\n",
    "q = cvxopt.matrix(-np.ones((m, 1)), tc='d')\n",
    "G = cvxopt.matrix(-np.eye(m), tc='d')\n",
    "h = cvxopt.matrix(np.zeros(m), tc='d')\n",
    "A = cvxopt.matrix(y.reshape(1, -1).astype('double'))\n",
    "b = cvxopt.matrix(0.0)\n",
    "\n",
    "# Resolvemos el problema cuadrático usando cvxopt\n",
    "solution = cvxopt.solvers.qp(P, q, G, h, A, b)\n",
    "\n",
    "# Obtenemos los multiplicadores de Lagrange (alpha) de la solución\n",
    "alphas = np.array(solution['x']).flatten()\n",
    "alphas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39bd4f9",
   "metadata": {},
   "source": [
    "## Identificación de vectores soporte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4125ebc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Filtramos los vectores soporte con un umbral para alphas mayores que cero\n",
    "support_vector_indices = np.where(alphas > 1e-5)[0]\n",
    "support_vectors = X[support_vector_indices]\n",
    "support_labels = y[support_vector_indices]\n",
    "support_alphas = alphas[support_vector_indices]\n",
    "support_vectors, support_alphas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff893247",
   "metadata": {},
   "source": [
    "## Cálculo del vector de pesos y término independiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e3c82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cálculo del vector de pesos theta*\n",
    "theta_star = np.sum((support_alphas * support_labels)[:, np.newaxis] * support_vectors, axis=0)\n",
    "\n",
    "# Cálculo del término independiente b* usando el primer vector soporte\n",
    "b_star = support_labels[0] - np.dot(theta_star, support_vectors[0])\n",
    "theta_star, b_star\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82c922c",
   "metadata": {},
   "source": [
    "## Clasificación de una nueva muestra [1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305ae296",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Clasificación de la nueva muestra\n",
    "new_sample = np.array([1, 1])\n",
    "decision_value = np.dot(theta_star, new_sample) + b_star\n",
    "classification = 1 if decision_value > 0 else -1\n",
    "classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00097a39",
   "metadata": {},
   "source": [
    "## Visualización de la frontera de decisión y vectores soporte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5bcf7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generamos el espacio para la frontera de decisión\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))\n",
    "Z = (theta_star[0] * xx + theta_star[1] * yy + b_star)\n",
    "\n",
    "# Realizamos el plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Puntos de entrenamiento\n",
    "plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], color='blue', marker='o', label='Clase +1')\n",
    "plt.scatter(X[y == -1][:, 0], X[y == -1][:, 1], color='red', marker='s', label='Clase -1')\n",
    "\n",
    "# Vectores soporte\n",
    "plt.scatter(support_vectors[:, 0], support_vectors[:, 1], s=100, facecolors='none', edgecolors='black', label='Vectores Soporte')\n",
    "\n",
    "# Frontera de decisión y márgenes\n",
    "plt.contour(xx, yy, Z, levels=[0], linewidths=2, colors='green')  # Línea de decisión\n",
    "plt.contour(xx, yy, Z, levels=[-1, 1], linestyles='--', colors='grey')  # Márgenes\n",
    "\n",
    "# Configuraciones adicionales\n",
    "plt.xlim(x_min, x_max)\n",
    "plt.ylim(y_min, y_max)\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.legend()\n",
    "plt.title('SVM: Frontera de Decisión y Vectores Soporte')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf55243b",
   "metadata": {},
   "source": [
    "## Verificación de la condición de KKT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38e31a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Verificamos la condición de KKT\n",
    "kkt_sum = np.sum(alphas * y)\n",
    "kkt_sum\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
